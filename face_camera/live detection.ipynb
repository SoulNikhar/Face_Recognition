{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0d3a97-ff47-450a-b03a-d34aeb6c9c2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# live face detection + big bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79ecfa-5ef3-4ed0-b0eb-7be471c7f3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    # Load a sample picture and learn how to recognize it\n",
    "    obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "    obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "    # Load a second sample picture and learn how to recognize it\n",
    "    biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "\n",
    "    # Create arrays of known face encodings and their names\n",
    "    known_face_encodings = [\n",
    "        obama_face_encoding,\n",
    "        biden_face_encoding\n",
    "    ]\n",
    "    known_face_names = [\n",
    "        \"Barack Obama\",\n",
    "        \"Joe Biden\"\n",
    "    ]\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "known_face_encodings, known_face_names = load_known_faces()\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # Find the best match if any\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances) if matches else None\n",
    "            \n",
    "            if best_match_index is not None and matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        color = (0, 0, 255) if name == \"Unknown\" else (0, 255, 0)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525693f4-36de-40f8-8be3-a1a68cfcbcdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# checking why above code is not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d61d9-db72-4bfc-8e96-0c991c81993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face locations: [(142, 617, 409, 349)]\n",
      "Face encodings: [array([-0.0914344 ,  0.13086095,  0.01314385, -0.05788445,  0.01628965,\n",
      "        0.00041327, -0.08469851, -0.09900524,  0.17989591, -0.10539678,\n",
      "        0.24560224,  0.08059315, -0.2161147 , -0.13486721,  0.04742461,\n",
      "        0.12056788, -0.16367513, -0.07826022, -0.1122469 , -0.10610124,\n",
      "        0.03652948,  0.00634994,  0.10533702,  0.04300565, -0.12117673,\n",
      "       -0.33629149, -0.06974643, -0.18218073, -0.00158545, -0.1120832 ,\n",
      "       -0.09656743, -0.02059199, -0.18194009, -0.1091411 ,  0.02073221,\n",
      "       -0.02022129,  0.00240957, -0.00374015,  0.20474017,  0.0282058 ,\n",
      "       -0.11632427,  0.09632833,  0.01547976,  0.21318354,  0.28629938,\n",
      "        0.07692298, -0.01180618, -0.09913055,  0.10386178, -0.21633516,\n",
      "        0.07274053,  0.14290063,  0.08237933,  0.04238797,  0.09769628,\n",
      "       -0.18852283,  0.00360183,  0.08834425, -0.14143489,  0.00837216,\n",
      "        0.0078872 , -0.08102693, -0.04035496,  0.0387958 ,  0.20594732,\n",
      "        0.09965956, -0.1229291 , -0.05094442,  0.13211268, -0.02900139,\n",
      "        0.02445153,  0.02434404, -0.18431334, -0.20063369, -0.22774039,\n",
      "        0.09293823,  0.37345198,  0.19359806, -0.2088118 ,  0.01955765,\n",
      "       -0.19599999,  0.02415315,  0.06105619,  0.00819598, -0.07174452,\n",
      "       -0.13538505, -0.04118638,  0.05282182,  0.0822657 ,  0.03208514,\n",
      "       -0.04098899,  0.21506976, -0.03382806,  0.06236776,  0.01853621,\n",
      "        0.05682226, -0.15838756, -0.03170495, -0.16015227, -0.06845063,\n",
      "        0.01404157, -0.04203653,  0.03085331,  0.14781639, -0.23243298,\n",
      "        0.05921936,  0.00418688, -0.04666766,  0.0222913 ,  0.07022521,\n",
      "       -0.02721735, -0.03373824,  0.05814214, -0.23816805,  0.24889056,\n",
      "        0.23403469,  0.02495461,  0.17327937,  0.07225873,  0.03394287,\n",
      "       -0.01637957, -0.02267808, -0.18229848, -0.06459411,  0.06046797,\n",
      "        0.0755232 ,  0.0852315 ,  0.00671965])]\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "\n",
    "# Load a sample picture\n",
    "image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "\n",
    "# Find all face locations in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# Attempt to encode the faces\n",
    "face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "print(\"Face locations:\", face_locations)\n",
    "print(\"Face encodings:\", face_encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19226a-7030-4818-9819-2e293e0c9a30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# live face detection working + unknown name + low accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614cf545-42cd-474f-959b-c4f3eed1eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# Initialize the video capture from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "\n",
    "        try:\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "        except TypeError as e:\n",
    "            print(f\"TypeError: {e}\")\n",
    "            # Debugging information\n",
    "            print(f\"Input image shape: {rgb_small_frame.shape}\")\n",
    "            print(f\"Input image dtype: {rgb_small_frame.dtype}\")\n",
    "            print(f\"Face locations: {face_locations}\")\n",
    "            continue  # Skip the rest of the loop and proceed to the next frame\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # Here you would compare the face_encoding to known encodings to find a match\n",
    "            name = \"Unknown\"  # Placeholder name\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1af2c-b657-4494-97b9-be1fae8ad008",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Face Bluring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5f4021-bebc-4e19-88a6-aeea917f315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(small_frame, model=\"cnn\")\n",
    "\n",
    "    # Display the results\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Extract the region of the image that contains the face\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "        # Blur the face image\n",
    "        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n",
    "\n",
    "        # Put the blurred face region back into the frame image\n",
    "        frame[top:bottom, left:right] = face_image\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466ca8c-6efa-4bdb-846e-7942a96aaaeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  CNN efficency calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46f78d6-4812-4561-898e-ba800a8c730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7429823875427246\n",
      "1.7231721878051758\n",
      "1.7336506843566895\n",
      "1.6927144527435303\n",
      "1.458082675933838\n",
      "1.4628629684448242\n",
      "1.433781623840332\n",
      "1.4478425979614258\n",
      "1.444518804550171\n",
      "1.4337716102600098\n",
      "1.4489481449127197\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    a = time.time()\n",
    "    face_locations = face_recognition.face_locations(small_frame, model=\"cnn\")\n",
    "    b = time.time()\n",
    "\n",
    "    # Display the results\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Extract the region of the image that contains the face\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "        # Blur the face image\n",
    "        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n",
    "\n",
    "        # Put the blurred face region back into the frame image\n",
    "        frame[top:bottom, left:right] = face_image\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "    print(b-a)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a7fd1-86a5-4ea0-8db6-b35001c5ebf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7204fc3e-b240-4e24-95fe-3a80fdec34ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function time in module time:\n",
      "\n",
      "time(...)\n",
      "    time() -> floating point number\n",
      "    \n",
      "    Return the current time in seconds since the Epoch.\n",
      "    Fractions of a second may be present if the system clock provides them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "help(time.time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3aac4-9624-495d-a65d-c612db514ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f405847-dd01-4435-9078-e8f58f41e4c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# live capture using haar cascade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053c460d-8fce-4956-b91c-adac4b1b94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "    return faces\n",
    "    \n",
    "while True:\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "\n",
    "    # apply the function we created to the video frame\n",
    "    faces = detect_bounding_box(video_frame)  \n",
    "\n",
    "    cv2.imshow(\"My Face Detection Project\", video_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c61967-7340-417a-96bf-54239ba0ef72",
   "metadata": {},
   "source": [
    "# slightly optimized of above version (haar cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6247d841-7856-417f-b3a7-940e916219f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "def detect_bounding_box(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "    return faces\n",
    "    \n",
    "while True:\n",
    "    result, video_frame = video_capture.read()\n",
    "    if not result:\n",
    "        print(\"Error: Failed to capture video frame.\")\n",
    "        break\n",
    "\n",
    "    faces = detect_bounding_box(video_frame)  \n",
    "\n",
    "    cv2.imshow(\"My Face Detection Project\", video_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798889a6-e90c-488b-9d06-c1774ffdf6cf",
   "metadata": {},
   "source": [
    "#  haar cascade with naming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e10e9-66f7-4f11-b158-09eda8de6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Haar Cascade face detector\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Example to add known faces (replace with your own images and names)\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "known_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "known_face_encodings.append(face_recognition.face_encodings(known_image)[0])\n",
    "known_face_names.append(\"obama\")\n",
    "\n",
    "known_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "known_face_encodings.append(face_recognition.face_encodings(known_image)[0])\n",
    "known_face_names.append(\"biden\")\n",
    "\n",
    "def detect_and_recognize_faces(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "\n",
    "    face_names = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_image = vid[y:y+h, x:x+w]\n",
    "        rgb_face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Try to recognize the face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_face_image)\n",
    "\n",
    "        name = \"None\"\n",
    "        if face_encodings:\n",
    "            # Compare detected face with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        # Put the name below the face\n",
    "        cv2.putText(vid, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        face_names.append(name)\n",
    "        \n",
    "    return face_names\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()\n",
    "    if not result:\n",
    "        print(\"Error: Failed to capture video frame.\")\n",
    "        break\n",
    "\n",
    "    face_names = detect_and_recognize_faces(video_frame)  \n",
    "    print(f\"Detected Faces: {face_names if face_names else 'None'}\")\n",
    "\n",
    "    cv2.imshow(\"My Face Detection and Recognition Project\", video_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16afd85a-c585-4f3a-8a77-dff88cee31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Haar Cascade face detector\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Example to add known faces (replace with your own images and names)\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "known_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "known_face_encodings.append(face_recognition.face_encodings(known_image)[0])\n",
    "known_face_names.append(\"Person 1\")\n",
    "\n",
    "known_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "known_face_encodings.append(face_recognition.face_encodings(known_image)[0])\n",
    "known_face_names.append(\"Person 2\")\n",
    "\n",
    "def detect_and_recognize_faces(vid):\n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_image = vid[y:y+h, x:x+w]\n",
    "        rgb_face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Try to recognize the face\n",
    "        face_encodings = face_recognition.face_encodings(rgb_face_image)\n",
    "\n",
    "        name = \"None\"\n",
    "        if face_encodings:\n",
    "            # Compare detected face with known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encodings[0])\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        # Put the name or \"None\" inside the box\n",
    "        cv2.putText(vid, name, (x, y - 10 if y - 10 > 20 else y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "    return\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    result, video_frame = video_capture.read()\n",
    "    if not result:\n",
    "        print(\"Error: Failed to capture video frame.\")\n",
    "        break\n",
    "\n",
    "    detect_and_recognize_faces(video_frame)  \n",
    "\n",
    "    cv2.imshow(\"My Face Detection and Recognition Project\", video_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b07c0-fab1-4a90-a61b-fa1c332ffca6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  Using HOG algorithm but gives low accuracy +single face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4c4e8f-9642-4eb3-aebb-96f5c0d7409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "known_image_1 = face_recognition.load_image_file(\"biden.jpg\")\n",
    "known_encoding_1 = face_recognition.face_encodings(known_image_1)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "known_image_2 = face_recognition.load_image_file(\"obama.jpg\")\n",
    "known_encoding_2 = face_recognition.face_encodings(known_image_2)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    known_encoding_1,\n",
    "    known_encoding_2\n",
    "]\n",
    "known_face_names = [\n",
    "    \"biden 1\",\n",
    "    \"obama 2\"\n",
    "]\n",
    "\n",
    "# Initialize the video capture from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "            # If a match was found in known_face_encodings, use the first one.\n",
    "            name = \"Unknown\"\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425756e9-b20f-4b9e-8266-0c96bd841e73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HOG code with optimizations to skip more frames for faster performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d08cd1-06a2-4a71-ad97-264ebd30aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# Load sample pictures and learn how to recognize them\n",
    "known_image_1 = face_recognition.load_image_file(\"biden.jpg\")\n",
    "known_encoding_1 = face_recognition.face_encodings(known_image_1)[0]\n",
    "\n",
    "known_image_2 = face_recognition.load_image_file(\"obama.jpg\")\n",
    "known_encoding_2 = face_recognition.face_encodings(known_image_2)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    known_encoding_1,\n",
    "    known_encoding_2\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Biden\",\n",
    "    \"Obama\"\n",
    "]\n",
    "\n",
    "# Initialize the video capture from the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "frame_skip = 10  # Process every 10th frame\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    # Only process every nth frame for speed\n",
    "    if frame_count % frame_skip == 0:\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "            # If a match was found in known_face_encodings, use the first one.\n",
    "            name = \"Unknown\"\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49679c70-043c-4c6c-a742-353d984c7338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
