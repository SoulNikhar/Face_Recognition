{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0d3a97-ff47-450a-b03a-d34aeb6c9c2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# live face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b79ecfa-5ef3-4ed0-b0eb-7be471c7f3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x0000025FC6FEB430>, array([[[179, 181, 181],\n        [188, 182, 182],\n        [191, 183, 185],\n        ...,\n        [202, 187, 196],\n        [165, 155, 165],\n        [164, 154, 169]],\n\n       [[184, 185, 183],\n        [184, 184, 178],\n        [181, 181, 180],\n        ...,\n        [179, 160, 164],\n        [169, 155, 158],\n        [167, 158, 164]],\n\n       [[168, 178, 168],\n        [164, 173, 165],\n        [161, 164, 160],\n        ...,\n        [173, 154, 154],\n        [172, 158, 161],\n        [170, 159, 163]],\n\n       ...,\n\n       [[106,  90,  97],\n        [107,  94,  99],\n        [ 98,  91,  94],\n        ...,\n        [ 87,  67,  67],\n        [106,  87,  88],\n        [105,  83,  86]],\n\n       [[106,  93, 105],\n        [ 99,  92,  99],\n        [102,  98, 103],\n        ...,\n        [116,  93,  94],\n        [ 91,  71,  77],\n        [101,  87,  93]],\n\n       [[116, 100, 106],\n        [103,  94,  99],\n        [107,  99, 104],\n        ...,\n        [ 94,  74,  74],\n        [104,  89,  92],\n        [ 82,  71,  73]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x0000025FC6FE5D70>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Find all the faces and face encodings in the current frame of video\u001b[39;00m\n\u001b[0;32m     50\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(rgb_small_frame)\n\u001b[1;32m---> 51\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_small_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m face_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_encoding \u001b[38;5;129;01min\u001b[39;00m face_encodings:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# See if the face is a match for the known face(s)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\dummy\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\dummy\\lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x0000025FC6FEB430>, array([[[179, 181, 181],\n        [188, 182, 182],\n        [191, 183, 185],\n        ...,\n        [202, 187, 196],\n        [165, 155, 165],\n        [164, 154, 169]],\n\n       [[184, 185, 183],\n        [184, 184, 178],\n        [181, 181, 180],\n        ...,\n        [179, 160, 164],\n        [169, 155, 158],\n        [167, 158, 164]],\n\n       [[168, 178, 168],\n        [164, 173, 165],\n        [161, 164, 160],\n        ...,\n        [173, 154, 154],\n        [172, 158, 161],\n        [170, 159, 163]],\n\n       ...,\n\n       [[106,  90,  97],\n        [107,  94,  99],\n        [ 98,  91,  94],\n        ...,\n        [ 87,  67,  67],\n        [106,  87,  88],\n        [105,  83,  86]],\n\n       [[106,  93, 105],\n        [ 99,  92,  99],\n        [102,  98, 103],\n        ...,\n        [116,  93,  94],\n        [ 91,  71,  77],\n        [101,  87,  93]],\n\n       [[116, 100, 106],\n        [103,  94,  99],\n        [107,  99, 104],\n        ...,\n        [ 94,  74,  74],\n        [104,  89,  92],\n        [ 82,  71,  73]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x0000025FC6FE5D70>, 1"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    # Load a sample picture and learn how to recognize it\n",
    "    obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "    obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "    # Load a second sample picture and learn how to recognize it\n",
    "    biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "\n",
    "    # Create arrays of known face encodings and their names\n",
    "    known_face_encodings = [\n",
    "        obama_face_encoding,\n",
    "        biden_face_encoding\n",
    "    ]\n",
    "    known_face_names = [\n",
    "        \"Barack Obama\",\n",
    "        \"Joe Biden\"\n",
    "    ]\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "known_face_encodings, known_face_names = load_known_faces()\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # Find the best match if any\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances) if matches else None\n",
    "            \n",
    "            if best_match_index is not None and matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        color = (0, 0, 255) if name == \"Unknown\" else (0, 255, 0)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525693f4-36de-40f8-8be3-a1a68cfcbcdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# check why above code is not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1d61d9-db72-4bfc-8e96-0c991c81993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face locations: [(142, 617, 409, 349)]\n",
      "Face encodings: [array([-0.0914344 ,  0.13086095,  0.01314385, -0.05788445,  0.01628965,\n",
      "        0.00041327, -0.08469851, -0.09900524,  0.17989591, -0.10539678,\n",
      "        0.24560224,  0.08059315, -0.2161147 , -0.13486721,  0.04742461,\n",
      "        0.12056788, -0.16367513, -0.07826022, -0.1122469 , -0.10610124,\n",
      "        0.03652948,  0.00634994,  0.10533702,  0.04300565, -0.12117673,\n",
      "       -0.33629149, -0.06974643, -0.18218073, -0.00158545, -0.1120832 ,\n",
      "       -0.09656743, -0.02059199, -0.18194009, -0.1091411 ,  0.02073221,\n",
      "       -0.02022129,  0.00240957, -0.00374015,  0.20474017,  0.0282058 ,\n",
      "       -0.11632427,  0.09632833,  0.01547976,  0.21318354,  0.28629938,\n",
      "        0.07692298, -0.01180618, -0.09913055,  0.10386178, -0.21633516,\n",
      "        0.07274053,  0.14290063,  0.08237933,  0.04238797,  0.09769628,\n",
      "       -0.18852283,  0.00360183,  0.08834425, -0.14143489,  0.00837216,\n",
      "        0.0078872 , -0.08102693, -0.04035496,  0.0387958 ,  0.20594732,\n",
      "        0.09965956, -0.1229291 , -0.05094442,  0.13211268, -0.02900139,\n",
      "        0.02445153,  0.02434404, -0.18431334, -0.20063369, -0.22774039,\n",
      "        0.09293823,  0.37345198,  0.19359806, -0.2088118 ,  0.01955765,\n",
      "       -0.19599999,  0.02415315,  0.06105619,  0.00819598, -0.07174452,\n",
      "       -0.13538505, -0.04118638,  0.05282182,  0.0822657 ,  0.03208514,\n",
      "       -0.04098899,  0.21506976, -0.03382806,  0.06236776,  0.01853621,\n",
      "        0.05682226, -0.15838756, -0.03170495, -0.16015227, -0.06845063,\n",
      "        0.01404157, -0.04203653,  0.03085331,  0.14781639, -0.23243298,\n",
      "        0.05921936,  0.00418688, -0.04666766,  0.0222913 ,  0.07022521,\n",
      "       -0.02721735, -0.03373824,  0.05814214, -0.23816805,  0.24889056,\n",
      "        0.23403469,  0.02495461,  0.17327937,  0.07225873,  0.03394287,\n",
      "       -0.01637957, -0.02267808, -0.18229848, -0.06459411,  0.06046797,\n",
      "        0.0755232 ,  0.0852315 ,  0.00671965])]\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "\n",
    "# Load a sample picture\n",
    "image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "\n",
    "# Find all face locations in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# Attempt to encode the faces\n",
    "face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "print(\"Face locations:\", face_locations)\n",
    "print(\"Face encodings:\", face_encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1af2c-b657-4494-97b9-be1fae8ad008",
   "metadata": {},
   "source": [
    "# Face Bluring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f4021-bebc-4e19-88a6-aeea917f315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(small_frame, model=\"cnn\")\n",
    "\n",
    "    # Display the results\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Extract the region of the image that contains the face\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "        # Blur the face image\n",
    "        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n",
    "\n",
    "        # Put the blurred face region back into the frame image\n",
    "        frame[top:bottom, left:right] = face_image\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc644db-5ea3-4334-a106-5d26fca908cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# new check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea62e08-7b55-4b8c-8e53-57397b8691e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load known faces and their encodings\n",
    "def load_known_faces():\n",
    "    try:\n",
    "        obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "        obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "        biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "        biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "\n",
    "        known_face_encodings = [obama_face_encoding, biden_face_encoding]\n",
    "        known_face_names = [\"Barack Obama\", \"Joe Biden\"]\n",
    "        return known_face_encodings, known_face_names\n",
    "    except IndexError as e:\n",
    "        print(\"Error loading known face images: \", e)\n",
    "        return [], []\n",
    "\n",
    "known_face_encodings, known_face_names = load_known_faces()\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    if process_this_frame:\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances) if matches else None\n",
    "            \n",
    "            if best_match_index is not None and matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        color = (0, 0, 255) if name == \"Unknown\" else (0, 255, 0)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737b417-9172-4aae-8d2b-8f428ee6402d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
